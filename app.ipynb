{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e715da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 11:57:33.742640: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-18 11:57:33.752190: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744977453.765686  171041 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744977453.769182  171041 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744977453.778910  171041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744977453.778928  171041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744977453.778929  171041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744977453.778930  171041 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-18 11:57:33.783010: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for required files...\n",
      "Loaded model parameters. Using max_length=40\n",
      "Loaded tokenizers successfully.\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744977455.962708  171041 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3539 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/home/wsl/myenv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 11 variables whereas the saved optimizer has 20 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "\n",
      "----- Testing English to Bengali Translation -----\n",
      "\n",
      "===== English to Bengali Translation Interface =====\n",
      "Options:\n",
      "1. Single sentence translation\n",
      "2. Batch translation from file\n",
      "3. Save translations to file\n",
      "4. Adjust beam width (currently: 3)\n",
      "5. Toggle verbose mode (currently OFF)\n",
      "6. Exit\n",
      "\n",
      "Original: \"a little girl climbed up a tree\"\n",
      "Bengali translation: \"একটি ছোট মেয়ে গাছে উঠছে\"\n",
      "\n",
      "Original: \"i love you\"\n",
      "Bengali translation: \"আমি আপনাকে ভালোবাসি\"\n",
      "\n",
      "Original: \"you are brave\"\n",
      "Bengali translation: \"আপনারা চিৎকার করছেন\"\n",
      "\n",
      "Exiting translation interface. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define the Tokenize class to match what was used during training\n",
    "class Tokenize:\n",
    "    def __init__(self, max_tokens=None, is_bengali=False):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.is_bengali = is_bengali\n",
    "        self.word_to_idx = {\n",
    "            '<PAD>': 0,\n",
    "            '<START>': 1,\n",
    "            '<END>': 2,\n",
    "            '<UNK>': 3\n",
    "        }\n",
    "        self.idx_to_word = {\n",
    "            0: '<PAD>',\n",
    "            1: '<START>',\n",
    "            2: '<END>',\n",
    "            3: '<UNK>'\n",
    "        }\n",
    "        self.len_vocab = 4\n",
    "        self.fit_done = False\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Build vocabulary from texts\"\"\"\n",
    "        word_counts = Counter()\n",
    "        \n",
    "        for text in texts:\n",
    "            # Clean text based on language\n",
    "            cleaned = clean_text(text, self.is_bengali)\n",
    "            tokens = cleaned.split()\n",
    "            word_counts.update(tokens)\n",
    "        \n",
    "        # Get most common words if max_tokens is set\n",
    "        if self.max_tokens:\n",
    "            most_common = word_counts.most_common(self.max_tokens - 4)  # -4 for special tokens\n",
    "        else:\n",
    "            most_common = word_counts.most_common()\n",
    "        \n",
    "        # Add words to vocabulary\n",
    "        for word, _ in most_common:\n",
    "            if word not in self.word_to_idx:\n",
    "                self.word_to_idx[word] = self.len_vocab\n",
    "                self.idx_to_word[self.len_vocab] = word\n",
    "                self.len_vocab += 1\n",
    "                \n",
    "        self.fit_done = True\n",
    "    \n",
    "    def __call__(self, texts):\n",
    "        \"\"\"Convert texts to tokens and ids\"\"\"\n",
    "        if not self.fit_done:\n",
    "            raise ValueError(\"You must call fit() before tokenizing texts\")\n",
    "        \n",
    "        all_tokens = []\n",
    "        all_ids = []\n",
    "        \n",
    "        for text in texts:\n",
    "            # Clean text based on language\n",
    "            cleaned = clean_text(text, self.is_bengali)\n",
    "            tokens = cleaned.split()\n",
    "            \n",
    "            # Add start and end tokens\n",
    "            tokens = ['<START>'] + tokens + ['<END>']\n",
    "            \n",
    "            # Convert to ids\n",
    "            ids = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) for token in tokens]\n",
    "            \n",
    "            all_tokens.append(tokens)\n",
    "            all_ids.append(ids)\n",
    "            \n",
    "        return all_tokens, all_ids\n",
    "\n",
    "# Define clean_text function to match what was used during training\n",
    "def clean_text(text, is_bengali=False):\n",
    "    \"\"\"\n",
    "    Clean text by removing punctuation and normalizing whitespace.\n",
    "    Handles non-string inputs safely.\n",
    "    \"\"\"\n",
    "    # Handle non-string inputs (NaN values, integers, etc.)\n",
    "    if not isinstance(text, str):\n",
    "        if hasattr(text, 'isna') and text.isna():  # Handle NaN values\n",
    "            return \"\"\n",
    "        # Convert numbers or other types to string\n",
    "        text = str(text)\n",
    "    \n",
    "    if not is_bengali:\n",
    "        # English cleaning\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "    else:\n",
    "        # Bengali cleaning: remove punctuation, keep Bengali chars\n",
    "        text = re.sub(r\"[^\\u0980-\\u09FF\\s]\", \" \", text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# Define masked loss function\n",
    "def masked_loss(y_true, y_pred):\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "    loss = loss_fn(y_true, y_pred)\n",
    "    \n",
    "    mask = tf.cast(y_true != 0, loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "\n",
    "# Define masked accuracy function\n",
    "def masked_acc(y_true, y_pred):\n",
    "    y_pred = tf.argmax(y_pred, axis=-1)\n",
    "    y_pred = tf.cast(y_pred, y_true.dtype)\n",
    "    \n",
    "    match = tf.cast(y_true == y_pred, tf.float32)\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    \n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)\n",
    "\n",
    "# Define prediction function with beam search (similar to original notebook)\n",
    "def predict_with_beam_search(text, model, tokenize_en, tokenize_bn, max_len, clean_text_func, start_token, end_token, beam_width=3):\n",
    "    \"\"\"\n",
    "    Prediction function with beam search for better translation quality\n",
    "    \n",
    "    Args:\n",
    "        text: Input English text\n",
    "        model: Loaded translation model\n",
    "        tokenize_en: English tokenizer\n",
    "        tokenize_bn: Bengali tokenizer\n",
    "        max_len: Maximum sequence length\n",
    "        clean_text_func: Text cleaning function\n",
    "        start_token: Start token string\n",
    "        end_token: End token string\n",
    "        beam_width: Width of beam search (higher = more candidates)\n",
    "        \n",
    "    Returns:\n",
    "        Best Bengali translation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Clean and tokenize input text\n",
    "        clean_text_en = clean_text_func(text)\n",
    "        _, sent_id_en = tokenize_en([clean_text_en])\n",
    "        sent_id_en = sent_id_en[0]\n",
    "        \n",
    "        # Pad encoder input\n",
    "        sent_id_en_pad = sent_id_en[:max_len] if len(sent_id_en) > max_len else sent_id_en + [0] * (max_len - len(sent_id_en))\n",
    "        sent_id_en_pad = np.array([sent_id_en_pad], dtype=np.int32)\n",
    "        \n",
    "        # Initialize decoder input with start token\n",
    "        sent_id_bn = np.zeros(max_len, dtype=np.int32)\n",
    "        sent_id_bn[0] = tokenize_bn.word_to_idx[start_token]\n",
    "        \n",
    "        # Beam search parameters\n",
    "        candidates = [(sent_id_bn, 0.0)]  # (sequence, score)\n",
    "        \n",
    "        for i in range(max_len - 1):\n",
    "            all_candidates = []\n",
    "            \n",
    "            # Expand each candidate\n",
    "            for seq, score in candidates:\n",
    "                # If sequence already has an end token, keep it as is\n",
    "                if i > 0 and seq[i] == tokenize_bn.word_to_idx[end_token]:\n",
    "                    all_candidates.append((seq, score))\n",
    "                    continue\n",
    "                \n",
    "                # Predict next token probabilities\n",
    "                test_predict = model.predict(\n",
    "                    [np.array([sent_id_en_pad[0]]), np.array([seq])], \n",
    "                    verbose=0)\n",
    "                \n",
    "                # Get top k predictions\n",
    "                top_preds = np.argsort(test_predict[0][i])[-beam_width:]\n",
    "                \n",
    "                # Create new candidates\n",
    "                for p in top_preds:\n",
    "                    new_seq = seq.copy()\n",
    "                    new_seq[i+1] = p\n",
    "                    # Add log probability to score\n",
    "                    new_score = score + np.log(test_predict[0][i][p] + 1e-10)\n",
    "                    all_candidates.append((new_seq, new_score))\n",
    "            \n",
    "            # Select top candidates\n",
    "            candidates = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            \n",
    "            # Check if all candidates have ended\n",
    "            if all(seq[i+1] == tokenize_bn.word_to_idx[end_token] for seq, _ in candidates):\n",
    "                break\n",
    "        \n",
    "        # Select best candidate\n",
    "        best_seq, _ = candidates[0]\n",
    "        \n",
    "        # Extract result (ignore start token, stop at end token or padding)\n",
    "        result = []\n",
    "        for j in range(1, max_len):\n",
    "            if best_seq[j] == tokenize_bn.word_to_idx[end_token] or best_seq[j] == 0:\n",
    "                break\n",
    "            result.append(tokenize_bn.idx_to_word[best_seq[j]])\n",
    "        \n",
    "        return ' '.join(result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during beam search: {e}\")\n",
    "        return \"Translation failed.\"\n",
    "\n",
    "# Constants\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD_TOKEN = '<PAD>'\n",
    "MAX_LENGTH = 40  # Default, will be overridden if model_params is loaded\n",
    "BEAM_WIDTH = 3   # Default beam width for search\n",
    "\n",
    "# Check if required files exist\n",
    "print(\"Checking for required files...\")\n",
    "required_files = ['seq2seq_model.keras']\n",
    "missing_files = [f for f in required_files if not os.path.exists(f)]\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Error: Missing required files: {missing_files}\")\n",
    "    exit(1)\n",
    "\n",
    "# Try to load model parameters if available\n",
    "try:\n",
    "    if os.path.exists('model_params.pkl'):\n",
    "        with open('model_params.pkl', 'rb') as f:\n",
    "            model_params = pickle.load(f)\n",
    "        MAX_LENGTH = model_params.get('max_length', MAX_LENGTH)\n",
    "        print(f\"Loaded model parameters. Using max_length={MAX_LENGTH}\")\n",
    "    else:\n",
    "        print(f\"model_params.pkl not found. Using default max_length={MAX_LENGTH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load model parameters: {e}\")\n",
    "    print(f\"Using default max_length={MAX_LENGTH}\")\n",
    "\n",
    "# Load tokenizers if available, otherwise create new ones\n",
    "try:\n",
    "    tokenize_en = None\n",
    "    tokenize_bn = None\n",
    "    \n",
    "    if os.path.exists('tokenize_en.pkl') and os.path.exists('tokenize_bn.pkl'):\n",
    "        try:\n",
    "            with open('tokenize_en.pkl', 'rb') as f:\n",
    "                tokenize_en = pickle.load(f)\n",
    "            with open('tokenize_bn.pkl', 'rb') as f:\n",
    "                tokenize_bn = pickle.load(f)\n",
    "            print(\"Loaded tokenizers successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading tokenizers: {e}\")\n",
    "            print(\"Will create tokenizers from scratch.\")\n",
    "    \n",
    "    if tokenize_en is None or tokenize_bn is None:\n",
    "        print(\"Creating new tokenizers (Note: these won't match the trained model's vocabulary)\")\n",
    "        # Create minimal tokenizers for testing only\n",
    "        tokenize_en = Tokenize(is_bengali=False)\n",
    "        tokenize_bn = Tokenize(is_bengali=True)\n",
    "        \n",
    "        # Add minimal vocabulary\n",
    "        for tokenizer in [tokenize_en, tokenize_bn]:\n",
    "            tokenizer.fit_done = True  # Mark as fitted even though we're not really fitting\n",
    "        \n",
    "        print(\"Created test tokenizers with minimal vocabulary.\")\n",
    "        print(\"WARNING: Translations will be very limited without proper tokenizers!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up tokenizers: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Load the model\n",
    "print(\"Loading model...\")\n",
    "try:\n",
    "    # Custom objects dictionary for loading model with custom functions\n",
    "    custom_objects = {\n",
    "        'masked_loss': masked_loss,\n",
    "        'masked_acc': masked_acc\n",
    "    }\n",
    "    \n",
    "    model = tf.keras.models.load_model('seq2seq_model.keras', custom_objects=custom_objects)\n",
    "    print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Define a simple translation function using beam search\n",
    "def translate(english_text):\n",
    "    \"\"\"Translate English text to Bengali using beam search\"\"\"\n",
    "    return predict_with_beam_search(\n",
    "        english_text, \n",
    "        model, \n",
    "        tokenize_en, \n",
    "        tokenize_bn, \n",
    "        MAX_LENGTH, \n",
    "        clean_text, \n",
    "        START_TOKEN, \n",
    "        END_TOKEN,\n",
    "        beam_width=BEAM_WIDTH\n",
    "    )\n",
    "\n",
    "# Improved translation interface\n",
    "def run_translation_interface():\n",
    "    global BEAM_WIDTH\n",
    "    \"\"\"Run an advanced translation interface with multiple options\"\"\"\n",
    "    print(\"\\n===== English to Bengali Translation Interface =====\")\n",
    "    print(\"Options:\")\n",
    "    print(\"1. Single sentence translation\")\n",
    "    print(\"2. Batch translation from file\")\n",
    "    print(\"3. Save translations to file\")\n",
    "    print(\"4. Adjust beam width (currently: {})\".format(BEAM_WIDTH))\n",
    "    print(\"5. Toggle verbose mode (currently OFF)\")\n",
    "    print(\"6. Exit\")\n",
    "    \n",
    "    verbose_mode = False\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            choice = input(\"\\nEnter your choice (1-6): \")\n",
    "            \n",
    "            if choice == '1':\n",
    "                # Single sentence translation\n",
    "                text = input(\"\\nEnter English text to translate: \")\n",
    "                print(f\"\\nOriginal: \\\"{text}\\\"\")\n",
    "                \n",
    "                if verbose_mode:\n",
    "                    print(\"Processing translation with beam width {}...\".format(BEAM_WIDTH))\n",
    "                    # Show cleaned text in verbose mode\n",
    "                    cleaned = clean_text(text)\n",
    "                    print(f\"Cleaned text: \\\"{cleaned}\\\"\")\n",
    "                \n",
    "                translation = translate(text)\n",
    "                print(f\"Bengali translation: \\\"{translation}\\\"\")\n",
    "                \n",
    "            elif choice == '2':\n",
    "                # Batch translation from file\n",
    "                filename = input(\"\\nEnter input file path: \")\n",
    "                \n",
    "                try:\n",
    "                    with open(filename, 'r', encoding='utf-8') as f:\n",
    "                        lines = f.readlines()\n",
    "                    \n",
    "                    if not lines:\n",
    "                        print(\"File is empty.\")\n",
    "                        continue\n",
    "                    \n",
    "                    print(f\"\\nTranslating {len(lines)} lines from {filename} with beam width {BEAM_WIDTH}...\")\n",
    "                    translations = []\n",
    "                    \n",
    "                    for i, line in enumerate(lines):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "                            \n",
    "                        if verbose_mode:\n",
    "                            print(f\"\\nLine {i+1}: \\\"{line}\\\"\")\n",
    "                        \n",
    "                        translation = translate(line)\n",
    "                        translations.append((line, translation))\n",
    "                        \n",
    "                        if verbose_mode:\n",
    "                            print(f\"Translation: \\\"{translation}\\\"\")\n",
    "                    \n",
    "                    print(f\"\\nCompleted {len(translations)} translations.\")\n",
    "                    \n",
    "                    # Display first 5 translations\n",
    "                    print(\"\\nSample translations:\")\n",
    "                    for i, (original, translated) in enumerate(translations[:5]):\n",
    "                        print(f\"{i+1}. \\\"{original}\\\" → \\\"{translated}\\\"\")\n",
    "                    \n",
    "                    # Ask if user wants to save results\n",
    "                    save = input(\"\\nSave translations to file? (y/n): \")\n",
    "                    if save.lower() == 'y':\n",
    "                        output_file = input(\"Enter output file path: \")\n",
    "                        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                            for original, translated in translations:\n",
    "                                f.write(f\"{original}\\t{translated}\\n\")\n",
    "                        print(f\"Translations saved to {output_file}\")\n",
    "                        \n",
    "                except FileNotFoundError:\n",
    "                    print(f\"File not found: {filename}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing file: {e}\")\n",
    "                    \n",
    "            elif choice == '3':\n",
    "                # Save translations to file\n",
    "                output_file = input(\"\\nEnter output file path: \")\n",
    "                print(\"Enter English sentences (one per line). Type 'DONE' on a new line when finished.\")\n",
    "                \n",
    "                sentences = []\n",
    "                while True:\n",
    "                    line = input()\n",
    "                    if line == 'DONE':\n",
    "                        break\n",
    "                    sentences.append(line)\n",
    "                \n",
    "                if not sentences:\n",
    "                    print(\"No sentences provided.\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                        for sentence in sentences:\n",
    "                            translation = translate(sentence)\n",
    "                            f.write(f\"{sentence}\\t{translation}\\n\")\n",
    "                    print(f\"\\nTranslated {len(sentences)} sentences and saved to {output_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving translations: {e}\")\n",
    "            \n",
    "            elif choice == '4':\n",
    "                # Adjust beam width\n",
    "                try:\n",
    "                    new_width = int(input(f\"\\nCurrent beam width is {BEAM_WIDTH}. Enter new beam width (1-10): \"))\n",
    "                    if 1 <= new_width <= 10:\n",
    "                        BEAM_WIDTH = new_width\n",
    "                        print(f\"Beam width set to {BEAM_WIDTH}\")\n",
    "                    else:\n",
    "                        print(\"Beam width must be between 1 and 10\")\n",
    "                except ValueError:\n",
    "                    print(\"Please enter a valid number\")\n",
    "                    \n",
    "            elif choice == '5':\n",
    "                # Toggle verbose mode\n",
    "                verbose_mode = not verbose_mode\n",
    "                print(f\"Verbose mode turned {'ON' if verbose_mode else 'OFF'}\")\n",
    "                \n",
    "            elif choice == '6':\n",
    "                # Exit\n",
    "                print(\"\\nExiting translation interface. Goodbye!\")\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                print(\"Invalid choice. Please enter a number between 1 and 6.\")\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nOperation cancelled.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----- Testing English to Bengali Translation -----\")\n",
    "\n",
    "\n",
    "# Run the interface\n",
    "run_translation_interface()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
